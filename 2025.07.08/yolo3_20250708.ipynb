{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6587271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py:1183: UserWarning: Theme should be a class loaded from gradio.themes\n",
      "  warnings.warn(\"Theme should be a class loaded from gradio.themes\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/components/image.py:134: UserWarning: The `mirror_webcam` parameter is deprecated. Please use the `webcam_options` parameter with a `gr.WebcamOptions` instance instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLICK_SEND_GPT :  이 사진에서 YOLO 모델이 감지한 물체는 다음과 같습니다.\n",
      "\n",
      "- 감지된 물체: **Person(사람)**\n",
      "- 감지 확률: **99.9%**\n",
      "- 바운딩 박스 설명: 사진 중앙 아래쪽 잔디밭 위에 앉아 있는 사람(들)이 바운딩 박스 안에 있습니다. 해당 인물은 에펠탑 앞 잔디밭에서 휴식을 취하거나 대화를 나누고 있는 것으로 보입니다. 주변에 돗자리나 가방 등도 보이나, 바운딩 박스 안에는 주로 사람이 포함되어 있습니다.\n",
      "\n",
      "요약: 바운딩 박스 안에는 \"사람\"이 매우 높은 확률(99.9%)로 감지되었습니다.\n",
      "이 사진에서 YOLO 모델이 감지한 물체는 다음과 같습니다.\n",
      "\n",
      "- 감지된 물체: **Person(사람)**\n",
      "- 감지 확률: **99.9%**\n",
      "- 바운딩 박스 설명: 사진 중앙 아래쪽 잔디밭 위에 앉아 있는 사람(들)이 바운딩 박스 안에 있습니다. 해당 인물은 에펠탑 앞 잔디밭에서 휴식을 취하거나 대화를 나누고 있는 것으로 보입니다. 주변에 돗자리나 가방 등도 보이나, 바운딩 박스 안에는 주로 사람이 포함되어 있습니다.\n",
      "\n",
      "요약: 바운딩 박스 안에는 \"사람\"이 매우 높은 확률(99.9%)로 감지되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import io\n",
    "import requests\n",
    "import base64\n",
    "import datetime\n",
    "\n",
    "\n",
    "######################################\n",
    "# Azure 관련 전역 변수\n",
    "######################################\n",
    " \n",
    "OPENAI_ENDPOINT = \"YOUR_OPENAI_ENDPOINT\"    \n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "DEPLOYMENT_NAME = \"YOUR_DEPLOYMENT_NAME\"\n",
    " \n",
    "SPEECH_ENDPOINT = \"YOUR_SPEECH_ENDPOINT\"\n",
    "SPEECH_API_KEY = \"YOUR_SPEECH_API_KEY\"\n",
    " \n",
    "\n",
    "\n",
    "weights_path = \"yolo/yolov3.weights\"\n",
    "config_path = \"yolo/yolov3.cfg\"\n",
    "names_path = \"yolo/coco.names\"\n",
    "\n",
    "with open(names_path, 'r', encoding='utf-8') as file:\n",
    "    label_list = file.read().strip().split(\"\\n\")\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "def request_gpt(image_array):\n",
    "\n",
    "    # PIL 형태의 이미지\n",
    "    image = Image.fromarray(image_array)\n",
    "\n",
    "    byte_image = io.BytesIO()\n",
    "    image.save(byte_image, format=\"png\")\n",
    "    base64_image = base64.b64encode(byte_image.getvalue()).decode('utf-8')\n",
    "\n",
    "    endpoint = \"{}openai/deployments/{}/chat/completions?api-version=2025-01-01-preview\".format(OPENAI_ENDPOINT, DEPLOYMENT_NAME)\n",
    " \n",
    "    headers = {\n",
    "        \"Content-Type\":\"application/json\",\n",
    "        \"Authorization\":\"Bearer {}\".format(OPENAI_API_KEY)\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        너는 사진속에서 감지된 물체를 분석하는 AI 봇이야.\n",
    "                        무조건 분석 결과를 한국어로 답변해줘.\n",
    "                        \"\"\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        너는 물체를 감지하는 YOLO 모델이야.\n",
    "                        이 사진에서 감지된 물체에 대해서 감지확률과 자세한 설명을 붙여줘.\n",
    "                        반드시 감지된 물체, 바운딩 박스 안에 있는 물체에 대해서만 설명해줘.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 16000\n",
    "    }\n",
    " \n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "    return content\n",
    "\n",
    "def request_tts(text):\n",
    "    endpoint = SPEECH_ENDPOINT\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": SPEECH_API_KEY,\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\": \"riff-8khz-16bit-mono-pcm\"\n",
    "    }\n",
    "    \n",
    "    body = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\"\n",
    "        xmlns:mstts=\"http://www.w3.org/2001/mstts\" xml:lang=\"ko-KR\">\n",
    "    <voice name=\"ko-KR-SeoHyeonNeural\">\n",
    "        {text}\n",
    "    </voice>\n",
    "    </speak>\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, data=body)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    file_name = \"tts_result_{}.wav\".format(datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    \n",
    "    with open(file_name, \"wb\") as audio_file:\n",
    "        audio_file.write(response.content)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def random_color():\n",
    "    import random\n",
    "    # 랜덤한 RGB 색상 튜플 반환\n",
    "    return (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) \n",
    "\n",
    "def get_font():\n",
    "    # OS별로 적절한 폰트 객체 반환 (한글 지원)\n",
    "    from PIL import ImageFont\n",
    "    import platform\n",
    "    \n",
    "    font_size = 20\n",
    "    \n",
    "    try:\n",
    "        if platform.system() == \"Windows\":\n",
    "            # 윈도우용 한글 폰트\n",
    "            return ImageFont.truetype(\"malgun.ttf\", font_size)\n",
    "        elif platform.system() == \"Darwin\":  # macOS\n",
    "            # 맥용 한글 폰트\n",
    "            return ImageFont.truetype(\"AppleGothic.ttf\", font_size)\n",
    "        else:  # Linux      \n",
    "            # 리눅스 기본 폰트\n",
    "            return ImageFont.load_default(size=font_size)\n",
    "    except IOError:\n",
    "        # 폰트 파일이 없을 경우 기본 폰트 사용\n",
    "        return ImageFont.load_default(size=font_size)\n",
    "    \n",
    "\n",
    "def detect_object(image_array):\n",
    "\n",
    "    image = Image.fromarray(image_array.copy())\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = get_font()\n",
    "\n",
    "    height, width = image_array.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image_array, 1/255.0, (416, 416),swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_name_list = net.getLayerNames()\n",
    "    out_layer_list = net.getUnconnectedOutLayersNames()\n",
    "    detection_list = net.forward(out_layer_list)\n",
    "\n",
    "    bounding_box_list = list()\n",
    "    confidence_list = list()\n",
    "    label_index_list = list()\n",
    "\n",
    "    for prediction_list in detection_list:\n",
    "        color = random_color()\n",
    "        # yolo82, yolo94, yolo 106\n",
    "        for prediction in prediction_list:\n",
    "            score_list = prediction[5:]\n",
    "            label_index = np.argmax(score_list)\n",
    "            confidence = score_list[label_index]\n",
    "            if confidence > 0.9:\n",
    "                bounding_box = prediction[:4] * np.array([width, height, width, height])\n",
    "                center_x, center_y, w, h = bounding_box.astype('int')\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                # print(x, y, w, h)\n",
    "\n",
    "                bounding_box_list.append([x, y, w, h])\n",
    "                confidence_list.append(confidence)\n",
    "                label_index_list.append(label_index)\n",
    "\n",
    "                # draw.rectangle([(x, y), (x + w, y + h)], outline='red', width=2)\n",
    "\n",
    "    extracted_index_list = cv2.dnn.NMSBoxes(bounding_box_list, confidence_list, 0.5, 0.4)\n",
    "\n",
    "    for extracted_index in extracted_index_list:\n",
    "        x, y, w, h = bounding_box_list[extracted_index]\n",
    "        confidence = confidence_list[extracted_index]\n",
    "        label_index = label_index_list[extracted_index]\n",
    "        label_text = label_list[label_index]\n",
    "\n",
    "        draw.rectangle([(x, y), (x + w, y + h)], outline=color, width=2)\n",
    "        draw.text((x + 5, y + 5), \"{}({:.2f}%)\".format(label_text, confidence * 100), fill=color, font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft) as demo:\n",
    "\n",
    "    def stream_webcam(image):\n",
    "        result_image = detect_object(image)\n",
    "        return result_image\n",
    "    \n",
    "    def click_capture(image):\n",
    "        if image is None:\n",
    "            raise gr.Error(\"감지된 이미지가 없습니다.\", duration=3)\n",
    "        return image\n",
    "    \n",
    "    def click_send_gpt(image_array, histories):\n",
    "        content = request_gpt(image_array)\n",
    "        print(\"CLICK_SEND_GPT : \", content)\n",
    "        now = datetime.datetime.now()\n",
    "        label_text = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        histories.append({\"role\" : \"user\", \"content\" : gr.Image(label=label_text, value=image_array)})\n",
    "        histories.append({\"role\" : \"assistant\", \"content\" : content})\n",
    "        # TODO : 오른쪽에 사진, 왼쪽에 콘텐츠.\n",
    "        return histories\n",
    "    \n",
    "    def change_chatbot(historioes):\n",
    "        content = historioes[-1]['content']\n",
    "        print(content)\n",
    "        file_path = request_tts(content)\n",
    "        return file_path\n",
    "\n",
    "    with gr.Row():\n",
    "        webcam_image = gr.Image(label=\"실시간 화면\", sources=\"webcam\", width=480, height=270, mirror_webcam=False)\n",
    "        output_image = gr.Image(label=\"검출 화면\", type='pil', width=480, height=270)\n",
    "        output_capture_image = gr.Image(label=\"이상 징후 캡쳐 화면\", interactive=False, width=480, height=270)\n",
    "\n",
    "    with gr.Row():\n",
    "        capture_button = gr.Button(\"이상 징후 발생\")\n",
    "        send_gpt_button = gr.Button(\"분석\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"분석 결과\", type=\"messages\")\n",
    "    chatbot_audio = gr.Audio(label=\"분석 결과\", interactive=False, autoplay=True)\n",
    "\n",
    "    webcam_image.stream(stream_webcam, inputs=[webcam_image], outputs=[output_image])\n",
    "    capture_button.click(click_capture, inputs=[output_image], outputs=[output_capture_image])\n",
    "    send_gpt_button.click(click_send_gpt, inputs=[output_capture_image, chatbot] ,outputs=[chatbot])\n",
    "    \n",
    "    chatbot.change(change_chatbot, inputs=[chatbot], outputs=[chatbot_audio])\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "# test_image = cv2.imread(\"/Users/yubin/Downloads/ImageTaggingSample1-fd324157.jpg\")\n",
    "# test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "# request_gpt(test_image)\n",
    "\n",
    "# detect_object(test_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
