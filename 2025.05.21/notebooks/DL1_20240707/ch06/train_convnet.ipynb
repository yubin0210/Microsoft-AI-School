{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspaces/DL-Exccersize/notebooks/DL_master_20240707/ch06\n",
            "/workspaces/DL-Exccersize/notebooks/DL_master_20240707\n",
            "train loss:2.300061998054956\n",
            "=== epoch:1, train acc:0.21, test acc:0.196 ===\n",
            "train loss:2.297471972541799\n",
            "train loss:2.294817959246826\n",
            "train loss:2.285731778726353\n",
            "train loss:2.2764528563173827\n",
            "train loss:2.2691441388074387\n",
            "train loss:2.263112342796445\n",
            "train loss:2.2384889639595387\n",
            "train loss:2.230192480131886\n",
            "train loss:2.199690548446269\n",
            "train loss:2.170583856587566\n",
            "train loss:2.15662485642755\n",
            "train loss:2.099995629312573\n",
            "train loss:2.0002813170615745\n",
            "train loss:1.9822729310542584\n",
            "train loss:1.9128376386853996\n",
            "train loss:1.886686701587083\n",
            "train loss:1.787201907355224\n",
            "train loss:1.6911068783183274\n",
            "train loss:1.624225058824322\n",
            "train loss:1.5099549983963276\n",
            "train loss:1.3949272444239045\n",
            "train loss:1.3460266389639932\n",
            "train loss:1.2593135266014372\n",
            "train loss:1.2045059135761567\n",
            "train loss:1.1786945903323665\n",
            "train loss:1.0867274511722709\n",
            "train loss:1.032987750592352\n",
            "train loss:0.9610243446506229\n",
            "train loss:0.8518543000834051\n",
            "train loss:0.7955044706639433\n",
            "train loss:0.8356946064087635\n",
            "train loss:0.8157165561517349\n",
            "train loss:0.7788190231992003\n",
            "train loss:0.6669965315690544\n",
            "train loss:0.6373900530955069\n",
            "train loss:0.7163896713735142\n",
            "train loss:0.6332626346848328\n",
            "train loss:0.5993644921426927\n",
            "train loss:0.7166844911805971\n",
            "train loss:0.6067046256235629\n",
            "train loss:0.6639057962050007\n",
            "train loss:0.6030560703296715\n",
            "train loss:0.6414893780846426\n",
            "train loss:0.5169681648818817\n",
            "train loss:0.5291981932735\n",
            "train loss:0.42570651299253165\n",
            "train loss:0.6690493023664434\n",
            "train loss:0.5049085551798215\n",
            "train loss:0.46959826793474746\n",
            "train loss:0.5262893484839143\n",
            "=== epoch:2, train acc:0.815, test acc:0.793 ===\n",
            "train loss:0.517269650339442\n",
            "train loss:0.4361760364822137\n",
            "train loss:0.4983410346175588\n",
            "train loss:0.46359454294098135\n",
            "train loss:0.5922546505514183\n",
            "train loss:0.5195847780977397\n",
            "train loss:0.4939503157744031\n",
            "train loss:0.4158299094343602\n",
            "train loss:0.3722537186252\n",
            "train loss:0.464353883876881\n",
            "train loss:0.4938521820019629\n",
            "train loss:0.3941523943620214\n",
            "train loss:0.5311734257010836\n",
            "train loss:0.27731839400819425\n",
            "train loss:0.43821202222832445\n",
            "train loss:0.39447443502260787\n",
            "train loss:0.4374686383300589\n",
            "train loss:0.4624589355537012\n",
            "train loss:0.414234039312793\n",
            "train loss:0.36178028919029054\n",
            "train loss:0.5189378541888554\n",
            "train loss:0.3629149597969478\n",
            "train loss:0.4651838513729312\n",
            "train loss:0.36273801925431015\n",
            "train loss:0.5345529544006236\n",
            "train loss:0.5102721922057302\n",
            "train loss:0.32073914565696443\n",
            "train loss:0.49195322227471017\n",
            "train loss:0.39376263114414845\n",
            "train loss:0.28712024883185183\n",
            "train loss:0.500378729290776\n",
            "train loss:0.3566777167415548\n",
            "train loss:0.36967940910225816\n",
            "train loss:0.44614904346075646\n",
            "train loss:0.5564216778662988\n",
            "train loss:0.4799909138002455\n",
            "train loss:0.33629370843659573\n",
            "train loss:0.5273557570969639\n",
            "train loss:0.2502389693859309\n",
            "train loss:0.2872040667769012\n",
            "train loss:0.5156036465048616\n",
            "train loss:0.39644975988408016\n",
            "train loss:0.4791092149809834\n",
            "train loss:0.36494254632387807\n",
            "train loss:0.2317001289006228\n",
            "train loss:0.3728624721825542\n",
            "train loss:0.2600356301114393\n",
            "train loss:0.5543517930567117\n",
            "train loss:0.3930999479981446\n",
            "train loss:0.2636795692318546\n",
            "=== epoch:3, train acc:0.872, test acc:0.859 ===\n",
            "train loss:0.4078910436530551\n",
            "train loss:0.5287696951684986\n",
            "train loss:0.3128967008841297\n",
            "train loss:0.3219041446852689\n",
            "train loss:0.24139558480463902\n",
            "train loss:0.30859707730095254\n",
            "train loss:0.24575205148222312\n",
            "train loss:0.41916344543384865\n",
            "train loss:0.4887386992139403\n",
            "train loss:0.248267307181151\n",
            "train loss:0.20534806309125034\n",
            "train loss:0.3688479568805512\n",
            "train loss:0.3191595634773305\n",
            "train loss:0.2533679050712807\n",
            "train loss:0.22347938478505772\n",
            "train loss:0.3261023828809498\n",
            "train loss:0.2442187268034336\n",
            "train loss:0.40956676924756785\n",
            "train loss:0.3319070828697273\n",
            "train loss:0.26431809037692977\n",
            "train loss:0.3160083539049196\n",
            "train loss:0.18116459875102858\n",
            "train loss:0.18016008932803096\n",
            "train loss:0.21896183837481636\n",
            "train loss:0.22875473966071636\n",
            "train loss:0.28845476762785927\n",
            "train loss:0.24243985965898307\n",
            "train loss:0.3009127327785711\n",
            "train loss:0.23057243139327321\n",
            "train loss:0.2874640386290507\n",
            "train loss:0.26753405325508944\n",
            "train loss:0.3778678055707288\n",
            "train loss:0.3350388152808454\n",
            "train loss:0.31430393174918375\n",
            "train loss:0.31507588741906495\n",
            "train loss:0.2710257210113111\n",
            "train loss:0.23914049903934143\n",
            "train loss:0.10471898742425587\n",
            "train loss:0.30137770426646066\n",
            "train loss:0.38102223703211685\n",
            "train loss:0.1680439847354203\n",
            "train loss:0.25991811908970913\n",
            "train loss:0.48272531261028767\n",
            "train loss:0.3014476580388702\n",
            "train loss:0.2332829806673222\n",
            "train loss:0.15272157833823352\n",
            "train loss:0.27224420798446136\n",
            "train loss:0.12933664159186908\n",
            "train loss:0.17118215405878418\n",
            "train loss:0.2849431895626737\n",
            "=== epoch:4, train acc:0.892, test acc:0.882 ===\n",
            "train loss:0.3050906870817686\n",
            "train loss:0.27035853703118723\n",
            "train loss:0.24325709612988622\n",
            "train loss:0.18890476028541936\n",
            "train loss:0.3236964087491721\n",
            "train loss:0.2619574101224258\n",
            "train loss:0.2678070349087615\n",
            "train loss:0.36917052223259356\n",
            "train loss:0.29376689892769164\n",
            "train loss:0.23264269569565105\n",
            "train loss:0.2323879085415906\n",
            "train loss:0.2818879497831814\n",
            "train loss:0.3322291698000439\n",
            "train loss:0.08069248851447634\n",
            "train loss:0.2267471750682376\n",
            "train loss:0.1680213505654274\n",
            "train loss:0.38240843595647056\n",
            "train loss:0.2782041590971936\n",
            "train loss:0.17783959540650474\n",
            "train loss:0.24951230103337366\n",
            "train loss:0.14386657130511643\n",
            "train loss:0.27724997049840716\n",
            "train loss:0.23381742105216355\n",
            "train loss:0.16655903960452936\n",
            "train loss:0.1947548200005883\n",
            "train loss:0.27170985405464504\n",
            "train loss:0.2995465880710254\n",
            "train loss:0.2549883726302089\n",
            "train loss:0.22872858452445446\n",
            "train loss:0.19871698734697918\n",
            "train loss:0.25175048899290947\n",
            "train loss:0.28824595198619046\n",
            "train loss:0.44288370532313254\n",
            "train loss:0.23823152062867567\n",
            "train loss:0.16368992847980102\n",
            "train loss:0.23098569288504733\n",
            "train loss:0.15324952321287733\n",
            "train loss:0.30181093017676713\n",
            "train loss:0.1727563647083041\n",
            "train loss:0.3238512955142625\n",
            "train loss:0.2834722230313097\n",
            "train loss:0.1982221303906708\n",
            "train loss:0.26976773276395155\n",
            "train loss:0.3133175259178403\n",
            "train loss:0.2553978715618779\n",
            "train loss:0.4157700432241084\n",
            "train loss:0.3550008223325953\n",
            "train loss:0.22014011932254107\n",
            "train loss:0.29053878749498485\n",
            "train loss:0.3597432137288319\n",
            "=== epoch:5, train acc:0.893, test acc:0.883 ===\n",
            "train loss:0.24533330122692582\n",
            "train loss:0.20886264325744905\n",
            "train loss:0.22010544090561218\n",
            "train loss:0.2609556686703435\n",
            "train loss:0.1670458682747525\n",
            "train loss:0.2844417963170291\n",
            "train loss:0.31240771775255644\n",
            "train loss:0.2973338961968715\n",
            "train loss:0.21626560440417691\n",
            "train loss:0.27512208457317056\n",
            "train loss:0.3356624875252372\n",
            "train loss:0.1456722253362294\n",
            "train loss:0.274380705794532\n",
            "train loss:0.24798875059531778\n",
            "train loss:0.10015084096513663\n",
            "train loss:0.23563701439191892\n",
            "train loss:0.27461668476635553\n",
            "train loss:0.1560214610931986\n",
            "train loss:0.14951533811833873\n",
            "train loss:0.1924120827032236\n",
            "train loss:0.13061936079094516\n",
            "train loss:0.2613804836753154\n",
            "train loss:0.20630180512456162\n",
            "train loss:0.2974195431942603\n",
            "train loss:0.20077151745233654\n",
            "train loss:0.22981800847020611\n",
            "train loss:0.19975802994994332\n",
            "train loss:0.13485098389551936\n",
            "train loss:0.12883855593407637\n",
            "train loss:0.20936554384138403\n",
            "train loss:0.11888754083290869\n",
            "train loss:0.3639755730437997\n",
            "train loss:0.20098346206533382\n",
            "train loss:0.1730083879004517\n",
            "train loss:0.25511369839673587\n",
            "train loss:0.2573929538751006\n",
            "train loss:0.21189196116951498\n",
            "train loss:0.3542543366806838\n",
            "train loss:0.29052754353401317\n",
            "train loss:0.11231449216834745\n",
            "train loss:0.17713628068472592\n",
            "train loss:0.22126684952522943\n",
            "train loss:0.17851908764109478\n",
            "train loss:0.2833093288378179\n",
            "train loss:0.1652593838643323\n",
            "train loss:0.1852577967185716\n",
            "train loss:0.2978252526076985\n",
            "train loss:0.18605185175891506\n",
            "train loss:0.18755777192782833\n",
            "train loss:0.18120123585744782\n",
            "=== epoch:6, train acc:0.923, test acc:0.909 ===\n",
            "train loss:0.34843896830536797\n",
            "train loss:0.23603165890561217\n",
            "train loss:0.24741869174354536\n",
            "train loss:0.14009762601928266\n",
            "train loss:0.10096610864754524\n",
            "train loss:0.34552766768303633\n",
            "train loss:0.16839534201307177\n",
            "train loss:0.12112759630313982\n",
            "train loss:0.18629487616762616\n",
            "train loss:0.20900814419709646\n",
            "train loss:0.17340163324160326\n",
            "train loss:0.11178362002536378\n",
            "train loss:0.20527670838454726\n",
            "train loss:0.21464230521419972\n",
            "train loss:0.16163293968412945\n",
            "train loss:0.3508634015995449\n",
            "train loss:0.18416452041964249\n",
            "train loss:0.10273848405295422\n",
            "train loss:0.2821774069563384\n",
            "train loss:0.18738516764202223\n",
            "train loss:0.19255978113673344\n",
            "train loss:0.17414628610665928\n",
            "train loss:0.2340607480280176\n",
            "train loss:0.11762428935762374\n",
            "train loss:0.07184340990003019\n",
            "train loss:0.23995949366934233\n",
            "train loss:0.26057027634229113\n",
            "train loss:0.09510410016297205\n",
            "train loss:0.05520947575520548\n",
            "train loss:0.09903487871504027\n",
            "train loss:0.1525576612095438\n",
            "train loss:0.2132560947882545\n",
            "train loss:0.2685038102304695\n",
            "train loss:0.14574595621994615\n",
            "train loss:0.14160126317989552\n",
            "train loss:0.3006142828558127\n",
            "train loss:0.17391837689796824\n",
            "train loss:0.22474130177629856\n",
            "train loss:0.12373669420258313\n",
            "train loss:0.19885743821576177\n",
            "train loss:0.1260951267222562\n",
            "train loss:0.15693061092712468\n",
            "train loss:0.17860682353838062\n",
            "train loss:0.15524357914486103\n",
            "train loss:0.1346296282831678\n",
            "train loss:0.42377792211485726\n",
            "train loss:0.29473715675799744\n",
            "train loss:0.16456639038054033\n",
            "train loss:0.11251543479416247\n",
            "train loss:0.2015086539278892\n",
            "=== epoch:7, train acc:0.936, test acc:0.909 ===\n",
            "train loss:0.14806410070557624\n",
            "train loss:0.07959646598546524\n",
            "train loss:0.16729682050966282\n",
            "train loss:0.30320860303425085\n",
            "train loss:0.22426075732862003\n",
            "train loss:0.2809256110997234\n",
            "train loss:0.12505115791673083\n",
            "train loss:0.1624386247037986\n",
            "train loss:0.14548594934144307\n",
            "train loss:0.17132471846835173\n",
            "train loss:0.20642775214475478\n",
            "train loss:0.15356270847214193\n",
            "train loss:0.19522116942925252\n",
            "train loss:0.15683098797752634\n",
            "train loss:0.25177481194703744\n",
            "train loss:0.1349314797187094\n",
            "train loss:0.149452540621521\n",
            "train loss:0.09158931338335502\n",
            "train loss:0.14488476491964652\n",
            "train loss:0.15451563467957238\n",
            "train loss:0.13433707170540415\n",
            "train loss:0.22230535109045482\n",
            "train loss:0.1696333100172552\n",
            "train loss:0.21432246136882382\n",
            "train loss:0.11201453277201764\n",
            "train loss:0.1150065417348001\n",
            "train loss:0.14385737272265467\n",
            "train loss:0.15680535650375838\n",
            "train loss:0.13098123898928354\n",
            "train loss:0.10152212430835492\n",
            "train loss:0.11736679267829292\n",
            "train loss:0.13102478890840905\n",
            "train loss:0.15156889752218727\n",
            "train loss:0.13659706099663826\n",
            "train loss:0.16173661176953133\n",
            "train loss:0.12721933605991506\n",
            "train loss:0.13427237678802084\n",
            "train loss:0.09257041870746914\n",
            "train loss:0.11873922735734686\n",
            "train loss:0.23390794255034134\n",
            "train loss:0.09505899330597899\n",
            "train loss:0.134176451490791\n",
            "train loss:0.07838548304363156\n",
            "train loss:0.1383475397883271\n",
            "train loss:0.11697502561772363\n",
            "train loss:0.10330248731629267\n",
            "train loss:0.11652012623955939\n",
            "train loss:0.14441085656176406\n",
            "train loss:0.158347073010936\n",
            "train loss:0.051089573911412306\n",
            "=== epoch:8, train acc:0.945, test acc:0.931 ===\n",
            "train loss:0.10846277487829022\n",
            "train loss:0.1982692763158399\n",
            "train loss:0.08494008159997767\n",
            "train loss:0.19551449315579938\n",
            "train loss:0.1448346292429534\n",
            "train loss:0.09795586753299305\n",
            "train loss:0.12135531883031708\n",
            "train loss:0.19187733313373706\n",
            "train loss:0.09402050806219081\n",
            "train loss:0.10811452308498967\n",
            "train loss:0.10631037697213708\n",
            "train loss:0.0937442595049639\n",
            "train loss:0.15090233918535498\n",
            "train loss:0.17012653732708485\n",
            "train loss:0.09508061380798292\n",
            "train loss:0.2218691871578713\n",
            "train loss:0.11473788806763227\n",
            "train loss:0.08862877725244385\n",
            "train loss:0.20543239968298266\n",
            "train loss:0.19821835896534307\n",
            "train loss:0.16086987940264502\n",
            "train loss:0.07289679798179546\n",
            "train loss:0.12816556583219607\n",
            "train loss:0.3380952048531423\n",
            "train loss:0.10992869723626746\n",
            "train loss:0.16173223985469282\n",
            "train loss:0.10679897665127022\n",
            "train loss:0.10877165794596401\n",
            "train loss:0.184654054921748\n",
            "train loss:0.08914152188123942\n",
            "train loss:0.13430250263187749\n",
            "train loss:0.13430678568240856\n",
            "train loss:0.14834407397002183\n",
            "train loss:0.09585275359954774\n",
            "train loss:0.09777177477829374\n",
            "train loss:0.13192883896244337\n",
            "train loss:0.10020597295310775\n",
            "train loss:0.08945973525148972\n",
            "train loss:0.07833488604494626\n",
            "train loss:0.11638919513542739\n",
            "train loss:0.22136565897651\n",
            "train loss:0.13469561757948786\n",
            "train loss:0.1130209847345853\n",
            "train loss:0.10999239733248332\n",
            "train loss:0.143663853292552\n",
            "train loss:0.10269898141447778\n",
            "train loss:0.15102051904484073\n",
            "train loss:0.06413437846837866\n",
            "train loss:0.14552172589868942\n",
            "train loss:0.19103876610911993\n",
            "=== epoch:9, train acc:0.946, test acc:0.935 ===\n",
            "train loss:0.19710367715321034\n",
            "train loss:0.0933137452895091\n",
            "train loss:0.10752650706796668\n",
            "train loss:0.13192232270522872\n",
            "train loss:0.12656093088939252\n",
            "train loss:0.12112664562377118\n",
            "train loss:0.1595950626261938\n",
            "train loss:0.11423343487742692\n",
            "train loss:0.10774244488077077\n",
            "train loss:0.07482587990276586\n",
            "train loss:0.1368456447330563\n",
            "train loss:0.09836036252719012\n",
            "train loss:0.11524394465105925\n",
            "train loss:0.1173799793521839\n",
            "train loss:0.1202685254191498\n",
            "train loss:0.1226158944227233\n",
            "train loss:0.13134778366127334\n",
            "train loss:0.15500151338276927\n",
            "train loss:0.17214190239444818\n",
            "train loss:0.08161001875363258\n",
            "train loss:0.036377967064536866\n",
            "train loss:0.11948279250186931\n",
            "train loss:0.13446612871740268\n",
            "train loss:0.09628370703255246\n",
            "train loss:0.16816437036769322\n",
            "train loss:0.12483200580134154\n",
            "train loss:0.1501220327171348\n",
            "train loss:0.09880742603015738\n",
            "train loss:0.07033893602944502\n",
            "train loss:0.07521256886709342\n",
            "train loss:0.13536214454947537\n",
            "train loss:0.15166136691732462\n",
            "train loss:0.12361427407058462\n",
            "train loss:0.15844029249420705\n",
            "train loss:0.04477483690651411\n",
            "train loss:0.05632031400270071\n",
            "train loss:0.08747949175024716\n",
            "train loss:0.031662206481059926\n",
            "train loss:0.08079520015015904\n",
            "train loss:0.103589511574725\n",
            "train loss:0.12947671457188267\n",
            "train loss:0.03812960466223648\n",
            "train loss:0.04921795896727354\n",
            "train loss:0.06648317484728503\n",
            "train loss:0.1345842519486797\n",
            "train loss:0.1343992229319252\n",
            "train loss:0.10531106214505283\n",
            "train loss:0.06184218194799557\n",
            "train loss:0.07920617506789143\n",
            "train loss:0.06211102798739468\n",
            "=== epoch:10, train acc:0.963, test acc:0.936 ===\n",
            "train loss:0.14755071156417235\n",
            "train loss:0.10354022811582843\n",
            "train loss:0.09943057216253831\n",
            "train loss:0.06739073677180311\n",
            "train loss:0.1330132185421214\n",
            "train loss:0.08605223816796687\n",
            "train loss:0.08843108862617419\n",
            "train loss:0.1268434763299745\n",
            "train loss:0.09238383411964594\n",
            "train loss:0.05962236586644975\n",
            "train loss:0.1496530184451114\n",
            "train loss:0.16450505574558283\n",
            "train loss:0.04274809113098956\n",
            "train loss:0.21499716286670836\n",
            "train loss:0.06355046128506714\n",
            "train loss:0.08510368042437527\n",
            "train loss:0.051190038290406366\n",
            "train loss:0.02756110208174619\n",
            "train loss:0.15256176809297073\n",
            "train loss:0.06331932133842011\n",
            "train loss:0.09934826471194298\n",
            "train loss:0.12258768839958142\n",
            "train loss:0.08813759985209206\n",
            "train loss:0.161190063278202\n",
            "train loss:0.06334469929704735\n",
            "train loss:0.06078314295572325\n",
            "train loss:0.132264468801857\n",
            "train loss:0.09882835565374765\n",
            "train loss:0.098518957738386\n",
            "train loss:0.1433789257487017\n",
            "train loss:0.041864247211657483\n",
            "train loss:0.06777562883556326\n",
            "train loss:0.10233067873217669\n",
            "train loss:0.14066921417361597\n",
            "train loss:0.16392958044294592\n",
            "train loss:0.04602152960412962\n",
            "train loss:0.09049593687089649\n",
            "train loss:0.0629646654052289\n",
            "train loss:0.1008538153316759\n",
            "train loss:0.09914411533684815\n",
            "train loss:0.08055127717759135\n",
            "train loss:0.07835821069335162\n",
            "train loss:0.0866498009796447\n",
            "train loss:0.07295161688277255\n",
            "train loss:0.07376128010588823\n",
            "train loss:0.08258043808937292\n",
            "train loss:0.06445198149581843\n",
            "train loss:0.07162973823098256\n",
            "train loss:0.09869786129667203\n",
            "train loss:0.03150076638294931\n",
            "=== epoch:11, train acc:0.96, test acc:0.946 ===\n",
            "train loss:0.16140423651542984\n",
            "train loss:0.044875445275341735\n",
            "train loss:0.04899551206308438\n",
            "train loss:0.034963051626061364\n",
            "train loss:0.052358560619937905\n",
            "train loss:0.041869851298248946\n",
            "train loss:0.08530586001742604\n",
            "train loss:0.049193179345049985\n",
            "train loss:0.12027046988902895\n",
            "train loss:0.09397950327632047\n",
            "train loss:0.0630364360105583\n",
            "train loss:0.06565261676565334\n",
            "train loss:0.04747185945370842\n",
            "train loss:0.11636354293458323\n",
            "train loss:0.11522599562668467\n",
            "train loss:0.10629140945974246\n",
            "train loss:0.12463059980518518\n",
            "train loss:0.061704840994446265\n",
            "train loss:0.15735770088197812\n",
            "train loss:0.11498197963151707\n",
            "train loss:0.08876936904334981\n",
            "train loss:0.07650354748311564\n",
            "train loss:0.08690602222366907\n",
            "train loss:0.0770849532298054\n",
            "train loss:0.04841952458177969\n",
            "train loss:0.05657501790228986\n",
            "train loss:0.07364984198521242\n",
            "train loss:0.057962541947440584\n",
            "train loss:0.05418591930383085\n",
            "train loss:0.12769755472352118\n",
            "train loss:0.08049154490273343\n",
            "train loss:0.11523940608370192\n",
            "train loss:0.07506079727677792\n",
            "train loss:0.08499337472866625\n",
            "train loss:0.06631608922936763\n",
            "train loss:0.08990747401944976\n",
            "train loss:0.19382159931175708\n",
            "train loss:0.08763529348204994\n",
            "train loss:0.1253398249486321\n",
            "train loss:0.0688238901664129\n",
            "train loss:0.07675047049613686\n",
            "train loss:0.10811857239842429\n",
            "train loss:0.055297132108224706\n",
            "train loss:0.08035452490584595\n",
            "train loss:0.06526515159106185\n",
            "train loss:0.09370844845954746\n",
            "train loss:0.08491467811776969\n",
            "train loss:0.03129907461646742\n",
            "train loss:0.09639181691175193\n",
            "train loss:0.07061756209863157\n",
            "=== epoch:12, train acc:0.968, test acc:0.957 ===\n",
            "train loss:0.02828099709197959\n",
            "train loss:0.05109314887430655\n",
            "train loss:0.09605537819492854\n",
            "train loss:0.05499539602707307\n",
            "train loss:0.057890498839913734\n",
            "train loss:0.04737695157283505\n",
            "train loss:0.0658254253991758\n",
            "train loss:0.05602476979300637\n",
            "train loss:0.13503115585521472\n",
            "train loss:0.03206065246849284\n",
            "train loss:0.06957106996994251\n",
            "train loss:0.05869951403932137\n",
            "train loss:0.06425646478616541\n",
            "train loss:0.05346587241408199\n",
            "train loss:0.04224557282469094\n",
            "train loss:0.05585868402506757\n",
            "train loss:0.0955505050615385\n",
            "train loss:0.03146031183591807\n",
            "train loss:0.037467455538107014\n",
            "train loss:0.07308360211968408\n",
            "train loss:0.03215144794454891\n",
            "train loss:0.03721392173915125\n",
            "train loss:0.13535054508109362\n",
            "train loss:0.05851365153516632\n",
            "train loss:0.0344348993704744\n",
            "train loss:0.06202792882897942\n",
            "train loss:0.022252711566778126\n",
            "train loss:0.15565986567692414\n",
            "train loss:0.06328320636276609\n",
            "train loss:0.04641034081635096\n",
            "train loss:0.09441142710266556\n",
            "train loss:0.07068614576801345\n",
            "train loss:0.14366601464706982\n",
            "train loss:0.06171309406671152\n",
            "train loss:0.06590307016815389\n",
            "train loss:0.07870901549025355\n",
            "train loss:0.02678915837586466\n",
            "train loss:0.042817721793328774\n",
            "train loss:0.08416099075886907\n",
            "train loss:0.1342914975238509\n",
            "train loss:0.03341405953353574\n",
            "train loss:0.04181738822762701\n",
            "train loss:0.020038410560346345\n",
            "train loss:0.09471268788527228\n",
            "train loss:0.057081095435642784\n",
            "train loss:0.04456570506431397\n",
            "train loss:0.09811057103638539\n",
            "train loss:0.06648141435827493\n",
            "train loss:0.14797961574686452\n",
            "train loss:0.03826543467261449\n",
            "=== epoch:13, train acc:0.972, test acc:0.947 ===\n",
            "train loss:0.03800702498650546\n",
            "train loss:0.03217545613747582\n",
            "train loss:0.052321186757375654\n",
            "train loss:0.0993598214384694\n",
            "train loss:0.09802760045897747\n",
            "train loss:0.04339721760153426\n",
            "train loss:0.059279097391585\n",
            "train loss:0.05657519049388608\n",
            "train loss:0.056018327287904054\n",
            "train loss:0.04180303690560444\n",
            "train loss:0.12457794289618368\n",
            "train loss:0.032085810659375893\n",
            "train loss:0.09585962385996734\n",
            "train loss:0.03839531518719726\n",
            "train loss:0.03914721008011308\n",
            "train loss:0.03352844129990509\n",
            "train loss:0.041609802754781884\n",
            "train loss:0.06699827974915792\n",
            "train loss:0.08477022740538763\n",
            "train loss:0.05051469345726947\n",
            "train loss:0.04880904556267241\n",
            "train loss:0.048708038278676605\n",
            "train loss:0.04530560556475204\n",
            "train loss:0.1125692138010877\n",
            "train loss:0.029734693763525163\n",
            "train loss:0.10391883106877778\n",
            "train loss:0.038591701376944884\n",
            "train loss:0.19878791646251043\n",
            "train loss:0.09650847586543103\n",
            "train loss:0.048371233113132926\n",
            "train loss:0.02826067103786637\n",
            "train loss:0.01888115059553579\n",
            "train loss:0.07363088952364004\n",
            "train loss:0.05369530008712383\n",
            "train loss:0.03247204589103669\n",
            "train loss:0.0166289249154172\n",
            "train loss:0.023770725995794086\n",
            "train loss:0.04647213238561587\n",
            "train loss:0.02943378910683452\n",
            "train loss:0.012219127703389787\n",
            "train loss:0.029800638979081126\n",
            "train loss:0.023745194916467774\n",
            "train loss:0.04278217805988262\n",
            "train loss:0.08015426437586604\n",
            "train loss:0.04130193018259919\n",
            "train loss:0.025923494422963985\n",
            "train loss:0.026244624939974263\n",
            "train loss:0.03762939335313591\n",
            "train loss:0.0523395431438308\n",
            "train loss:0.051505406010993855\n",
            "=== epoch:14, train acc:0.98, test acc:0.956 ===\n",
            "train loss:0.020992335580055105\n",
            "train loss:0.062026063207630856\n",
            "train loss:0.032738404761093715\n",
            "train loss:0.0623343841417726\n",
            "train loss:0.025849892020557937\n",
            "train loss:0.05677455455957272\n",
            "train loss:0.06841087633324343\n",
            "train loss:0.031592548488435984\n",
            "train loss:0.020575700504756444\n",
            "train loss:0.04382380169896139\n",
            "train loss:0.033214115919108805\n",
            "train loss:0.04127811640940396\n",
            "train loss:0.06416607667428817\n",
            "train loss:0.051457116828480906\n",
            "train loss:0.09670897220484947\n",
            "train loss:0.03730553249150217\n",
            "train loss:0.021854984581359573\n",
            "train loss:0.058122240604989474\n",
            "train loss:0.03054449074902206\n",
            "train loss:0.09150657743325702\n",
            "train loss:0.028098873791839677\n",
            "train loss:0.04774743018876492\n",
            "train loss:0.024968009376855\n",
            "train loss:0.07864865332332481\n",
            "train loss:0.05006407131257553\n",
            "train loss:0.06144766827021827\n",
            "train loss:0.016482407439084586\n",
            "train loss:0.01786950059210302\n",
            "train loss:0.02146607709814869\n",
            "train loss:0.03494514298943221\n",
            "train loss:0.06887385923603657\n",
            "train loss:0.03182226145139399\n",
            "train loss:0.04102169587244187\n",
            "train loss:0.06668766170057025\n",
            "train loss:0.03461982724156326\n",
            "train loss:0.01850584219291037\n",
            "train loss:0.025886266008207606\n",
            "train loss:0.03282066596208184\n",
            "train loss:0.04993231607430862\n",
            "train loss:0.05749853397720337\n",
            "train loss:0.032867600352719\n",
            "train loss:0.03211192436803231\n",
            "train loss:0.020084320206820126\n",
            "train loss:0.04709777658804939\n",
            "train loss:0.01607673492474724\n",
            "train loss:0.03079528187237736\n",
            "train loss:0.04776133379351286\n",
            "train loss:0.03755613811384671\n",
            "train loss:0.029077253442737542\n",
            "train loss:0.02491847206069317\n",
            "=== epoch:15, train acc:0.986, test acc:0.957 ===\n",
            "train loss:0.0156968868456091\n",
            "train loss:0.03335256724746835\n",
            "train loss:0.0714033970430499\n",
            "train loss:0.10755663655371038\n",
            "train loss:0.026886210364079483\n",
            "train loss:0.028647451066483854\n",
            "train loss:0.030432470511930907\n",
            "train loss:0.019031783551084314\n",
            "train loss:0.02068583845312364\n",
            "train loss:0.022403186174511674\n",
            "train loss:0.031156640624180377\n",
            "train loss:0.027389369923726488\n",
            "train loss:0.0345960760660742\n",
            "train loss:0.04250236557670847\n",
            "train loss:0.02022365156317828\n",
            "train loss:0.016856683266923772\n",
            "train loss:0.07485499504751733\n",
            "train loss:0.040698701487860055\n",
            "train loss:0.014850620194873987\n",
            "train loss:0.025036548482829747\n",
            "train loss:0.04890902513826781\n",
            "train loss:0.025459663589929557\n",
            "train loss:0.05361848141236034\n",
            "train loss:0.03186923262665825\n",
            "train loss:0.06930473165985054\n",
            "train loss:0.02860034715309071\n",
            "train loss:0.0308466504243657\n",
            "train loss:0.08909549048427509\n",
            "train loss:0.021669839084395872\n",
            "train loss:0.1498677437681035\n",
            "train loss:0.03177374948158213\n",
            "train loss:0.038777555856459235\n",
            "train loss:0.052423233996403334\n",
            "train loss:0.03658655786885444\n",
            "train loss:0.030604484546809772\n",
            "train loss:0.029838387242940932\n",
            "train loss:0.0375761418388999\n",
            "train loss:0.019114631689527536\n",
            "train loss:0.028688190499676462\n",
            "train loss:0.017417778369754597\n",
            "train loss:0.024687132740253533\n",
            "train loss:0.02193835482979445\n",
            "train loss:0.03681546570504876\n",
            "train loss:0.019981173457555516\n",
            "train loss:0.03410057671594696\n",
            "train loss:0.02271107895873997\n",
            "train loss:0.03610960761445533\n",
            "train loss:0.04159601267970689\n",
            "train loss:0.03225903133496744\n",
            "train loss:0.026612274026737933\n",
            "=== epoch:16, train acc:0.982, test acc:0.953 ===\n",
            "train loss:0.024350857882851275\n",
            "train loss:0.03269906924371768\n",
            "train loss:0.02444401616215172\n",
            "train loss:0.06168866994013436\n",
            "train loss:0.03039720097496809\n",
            "train loss:0.08766426195637646\n",
            "train loss:0.0731493127356611\n",
            "train loss:0.02295074940864035\n",
            "train loss:0.06386097597639012\n",
            "train loss:0.048576878753215445\n",
            "train loss:0.03505032686737868\n",
            "train loss:0.08379757279614002\n",
            "train loss:0.10854229178823178\n",
            "train loss:0.03722775285723354\n",
            "train loss:0.06709179778211119\n",
            "train loss:0.0807570195861176\n",
            "train loss:0.037422085874203724\n",
            "train loss:0.06754492329121316\n",
            "train loss:0.021785966496858326\n",
            "train loss:0.02436524555198281\n",
            "train loss:0.027420935746921678\n",
            "train loss:0.049624652163828796\n",
            "train loss:0.0500097507856819\n",
            "train loss:0.012631273255111496\n",
            "train loss:0.019954109567138424\n",
            "train loss:0.059056372552848534\n",
            "train loss:0.02616617762915916\n",
            "train loss:0.061934905728863975\n",
            "train loss:0.01926213786323231\n",
            "train loss:0.038861974133720494\n",
            "train loss:0.031545637359841956\n",
            "train loss:0.023479247325261698\n",
            "train loss:0.01962544687940374\n",
            "train loss:0.024903895976630515\n",
            "train loss:0.007015980280938984\n",
            "train loss:0.0448544697992242\n",
            "train loss:0.014088592127310609\n",
            "train loss:0.01507465119907541\n",
            "train loss:0.08499003753289362\n",
            "train loss:0.051284854456081164\n",
            "train loss:0.08649377566428926\n",
            "train loss:0.06219043936693851\n",
            "train loss:0.022226936436606545\n",
            "train loss:0.018107149930049176\n",
            "train loss:0.04420893563963431\n",
            "train loss:0.034740217974211965\n",
            "train loss:0.02751564625034364\n",
            "train loss:0.05936578260682911\n",
            "train loss:0.04473994165971175\n",
            "train loss:0.036001981769971964\n",
            "=== epoch:17, train acc:0.992, test acc:0.96 ===\n",
            "train loss:0.052412789159347245\n",
            "train loss:0.024268536753950003\n",
            "train loss:0.014882357525179905\n",
            "train loss:0.018170924083913798\n",
            "train loss:0.038518756583316945\n",
            "train loss:0.022874970326291055\n",
            "train loss:0.028266249940104182\n",
            "train loss:0.029356371977596482\n",
            "train loss:0.006741623215941466\n",
            "train loss:0.01765168845421525\n",
            "train loss:0.03448498232630748\n",
            "train loss:0.030769724734160033\n",
            "train loss:0.01813933579793459\n",
            "train loss:0.007128549600923691\n",
            "train loss:0.02230311963343168\n",
            "train loss:0.061767757363186127\n",
            "train loss:0.02175891196102675\n",
            "train loss:0.010409523062701416\n",
            "train loss:0.013793613351687004\n",
            "train loss:0.026097798802231703\n",
            "train loss:0.017783439406947286\n",
            "train loss:0.02520489944042002\n",
            "train loss:0.10680848088262543\n",
            "train loss:0.08455845342356849\n",
            "train loss:0.019329653251605617\n",
            "train loss:0.028586057960605583\n",
            "train loss:0.013887054025520585\n",
            "train loss:0.03671201451833338\n",
            "train loss:0.06346507379725556\n",
            "train loss:0.031199505369309138\n",
            "train loss:0.02847140570628233\n",
            "train loss:0.04755726900219334\n",
            "train loss:0.019560821367372912\n",
            "train loss:0.016464808636831644\n",
            "train loss:0.014383732250796871\n",
            "train loss:0.01593817955949152\n",
            "train loss:0.02450498307659818\n",
            "train loss:0.02228537281402781\n",
            "train loss:0.026755180538475577\n",
            "train loss:0.05257622798437926\n",
            "train loss:0.015895041741199267\n",
            "train loss:0.03737475477421947\n",
            "train loss:0.0784567475480713\n",
            "train loss:0.020796681544638443\n",
            "train loss:0.014830485766991197\n",
            "train loss:0.04068978793442939\n",
            "train loss:0.017529627713771957\n",
            "train loss:0.030669512591487997\n",
            "train loss:0.04414042815486506\n",
            "train loss:0.09513200796307501\n",
            "=== epoch:18, train acc:0.988, test acc:0.956 ===\n",
            "train loss:0.03170487522059875\n",
            "train loss:0.06638452772246227\n",
            "train loss:0.012759889089330624\n",
            "train loss:0.03579691058039607\n",
            "train loss:0.030083560873778455\n",
            "train loss:0.05869482357605824\n",
            "train loss:0.01139753396012785\n",
            "train loss:0.014414619521760876\n",
            "train loss:0.034058011049675284\n",
            "train loss:0.028039930525056492\n",
            "train loss:0.06265799383431916\n",
            "train loss:0.016840750100147815\n",
            "train loss:0.021190033514277392\n",
            "train loss:0.02385623557283659\n",
            "train loss:0.019712824849379407\n",
            "train loss:0.028645756417170473\n",
            "train loss:0.007810093546523106\n",
            "train loss:0.04734610296083273\n",
            "train loss:0.01211311507637937\n",
            "train loss:0.03575024442784818\n",
            "train loss:0.028402656404938678\n",
            "train loss:0.023950070443179254\n",
            "train loss:0.030485481388028642\n",
            "train loss:0.031004910318697333\n",
            "train loss:0.009012688633285568\n",
            "train loss:0.045796446513833224\n",
            "train loss:0.01684040672063715\n",
            "train loss:0.0340432009603202\n",
            "train loss:0.03807887802585528\n",
            "train loss:0.024793069235851818\n",
            "train loss:0.012541309286163286\n",
            "train loss:0.05059093546841401\n",
            "train loss:0.02018010263777459\n",
            "train loss:0.015828059743380408\n",
            "train loss:0.011721562138519001\n",
            "train loss:0.012997252289916208\n",
            "train loss:0.012812466138485155\n",
            "train loss:0.022568537540673724\n",
            "train loss:0.016178703204221146\n",
            "train loss:0.018718734690699526\n",
            "train loss:0.035676907398693114\n",
            "train loss:0.017442241402832053\n",
            "train loss:0.013014500640808013\n",
            "train loss:0.02146624955906668\n",
            "train loss:0.024751442366223408\n",
            "train loss:0.014829941635391228\n",
            "train loss:0.02219906115223145\n",
            "train loss:0.018485868751416792\n",
            "train loss:0.010920480498823389\n",
            "train loss:0.0312539448777386\n",
            "=== epoch:19, train acc:0.99, test acc:0.958 ===\n",
            "train loss:0.007540790606545229\n",
            "train loss:0.04610085807014634\n",
            "train loss:0.0107637287086207\n",
            "train loss:0.02453864853827581\n",
            "train loss:0.019483876026371413\n",
            "train loss:0.01207856991117076\n",
            "train loss:0.023110200051754414\n",
            "train loss:0.014597876586474174\n",
            "train loss:0.04687633432490781\n",
            "train loss:0.01995328350826011\n",
            "train loss:0.019988801913649715\n",
            "train loss:0.012053762607868628\n",
            "train loss:0.036597009140962665\n",
            "train loss:0.04985439367071272\n",
            "train loss:0.05170205870438174\n",
            "train loss:0.014932863532453815\n",
            "train loss:0.009401402340598119\n",
            "train loss:0.005141274660043806\n",
            "train loss:0.013629632642481786\n",
            "train loss:0.011828141309308633\n",
            "train loss:0.03375348804153218\n",
            "train loss:0.012537020749121903\n",
            "train loss:0.009701484877118482\n",
            "train loss:0.008812077956504016\n",
            "train loss:0.026188954043602285\n",
            "train loss:0.03456327408668155\n",
            "train loss:0.013553635696157062\n",
            "train loss:0.009957142674746048\n",
            "train loss:0.01317432149419941\n",
            "train loss:0.018162585876257343\n",
            "train loss:0.008823846918104814\n",
            "train loss:0.01731849744159723\n",
            "train loss:0.02552139961821982\n",
            "train loss:0.0076109657717514535\n",
            "train loss:0.02283546601956359\n",
            "train loss:0.03696108117170945\n",
            "train loss:0.018155888320756076\n",
            "train loss:0.022420678457435433\n",
            "train loss:0.009454998992462842\n",
            "train loss:0.011700962370871709\n",
            "train loss:0.007648307155373037\n",
            "train loss:0.02219320875844252\n",
            "train loss:0.014400863748692747\n",
            "train loss:0.021574483843110354\n",
            "train loss:0.026613622585114217\n",
            "train loss:0.021244933650114074\n",
            "train loss:0.009399702396474826\n",
            "train loss:0.011949147043931014\n",
            "train loss:0.008937484070690916\n",
            "train loss:0.01995310270470825\n",
            "=== epoch:20, train acc:0.993, test acc:0.965 ===\n",
            "train loss:0.04052630508455215\n",
            "train loss:0.02828045690360833\n",
            "train loss:0.009901018726164185\n",
            "train loss:0.0048035697510129165\n",
            "train loss:0.009928832967015522\n",
            "train loss:0.00965816198747208\n",
            "train loss:0.007469615458133471\n",
            "train loss:0.008729516155110334\n",
            "train loss:0.011749972387634495\n",
            "train loss:0.012954713071844839\n",
            "train loss:0.01142303201108978\n",
            "train loss:0.015593182564144636\n",
            "train loss:0.011230867450100608\n",
            "train loss:0.00718008077450351\n",
            "train loss:0.012598133284949633\n",
            "train loss:0.009116140545014716\n",
            "train loss:0.01646704455340886\n",
            "train loss:0.01903976475568029\n",
            "train loss:0.03762443948614829\n",
            "train loss:0.0053734330147373055\n",
            "train loss:0.011548371129603491\n",
            "train loss:0.01853935610936023\n",
            "train loss:0.007671959221775732\n",
            "train loss:0.007442565201553733\n",
            "train loss:0.01092231839741906\n",
            "train loss:0.01854981804564386\n",
            "train loss:0.02154362143007366\n",
            "train loss:0.011257543562053251\n",
            "train loss:0.03455231878114212\n",
            "train loss:0.009591447720159792\n",
            "train loss:0.017729558397370775\n",
            "train loss:0.03517967422718362\n",
            "train loss:0.041746451467841436\n",
            "train loss:0.006274386532278829\n",
            "train loss:0.012006492752690764\n",
            "train loss:0.017861867944238375\n",
            "train loss:0.006453454105930619\n",
            "train loss:0.06183969001326603\n",
            "train loss:0.0049504448972382264\n",
            "train loss:0.011046765376053692\n",
            "train loss:0.005497564103666899\n",
            "train loss:0.016895767148568677\n",
            "train loss:0.03511630823460087\n",
            "train loss:0.010526288230126126\n",
            "train loss:0.005725761225028966\n",
            "train loss:0.01873876804990432\n",
            "train loss:0.03776665636762342\n",
            "train loss:0.004622352286250398\n",
            "train loss:0.03200389309196184\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.962\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch06.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "#  \n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "#      .\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "#  \n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "#  \n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
